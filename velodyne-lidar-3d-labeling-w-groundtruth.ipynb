{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lidar 3D point cloud labeling with Velodyne Lidar sensor in SageMaker GroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to pre-process LiDAR sensor data to create an object tracking labeling job with Sensor fusion in SageMaker Ground Truth.\n",
    "\n",
    "For object tracking, you will track the movement of an object (e.g., car, pedestrian) while your point of reference (in this case, the car) is moving. You will experiment with coverting your 3D point cloud data from local coordinates to the world coordinate system to keep everything in the frame of reference.\n",
    "\n",
    "We will also include camera image leveraging the sensor fusion feature in SageMaker Ground Turth to provide labeling workers more visual information about the scene they are labeling. Through sensor fusion, workers will be able to adjust labels in the 3D scene and in 2D images, and label adjustments will be mirrored in the other view.\n",
    "\n",
    "The dataset used is provided to us by Velodyne. We will go over the dataset content in detail in later sections.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An S3 bucket you can write to. The bucket must be in the same region as this SageMaker Notebook instance. You can also define a valid S3 prefix. All the files related to this experiment will be stored in that prefix of your bucket. ***Important: you must attach the CORS policy to this bucket.** To learn how to add a CORS policy to your S3 bucket, follow the instructions in [How do I add cross-domain resource sharing with CORS?](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enabling-cors-examples.html). Paste the following policy in the CORS configuration editor:\n",
    "\n",
    "```\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n",
    "<CORSRule>\n",
    "    <AllowedOrigin>*</AllowedOrigin>\n",
    "    <AllowedMethod>GET</AllowedMethod>\n",
    "    <AllowedMethod>HEAD</AllowedMethod>\n",
    "    <AllowedMethod>PUT</AllowedMethod>\n",
    "    <MaxAgeSeconds>3000</MaxAgeSeconds>\n",
    "    <ExposeHeader>Access-Control-Allow-Origin</ExposeHeader>\n",
    "    <AllowedHeader>*</AllowedHeader>\n",
    "</CORSRule>\n",
    "<CORSRule>\n",
    "    <AllowedOrigin>*</AllowedOrigin>\n",
    "    <AllowedMethod>GET</AllowedMethod>\n",
    "</CORSRule>\n",
    "</CORSConfiguration>\n",
    "```\n",
    "\n",
    "- Download pcd.py (run the code block below): pyntcloud is a Python 3 library for working with 3D point clouds. This module allows us to work with the .pcd files generated from the LiDAR sensors\n",
    "- Familiarity with the [Ground Truth 3D Point Cloud Labeling Job](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html).\n",
    "- Familiarity with Python and numpy.\n",
    "- Basic understanding of [AWS Sagemaker](https://aws.amazon.com/sagemaker/).\n",
    "- Basic familiarity with [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/)\n",
    "\n",
    "This notebook has only been tested on a SageMaker notebook instance. We used an ml.t3.medium instance in our tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update awscli in case the version installed is out of date\n",
    "!pip install awscli --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and initialize parameters for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pcd\n",
    "import json\n",
    "import yaml\n",
    "import boto3\n",
    "import numpy as np\n",
    "import time\n",
    "import sagemaker\n",
    "from urllib.parse import urlparse\n",
    "from scipy.spatial.transform import Rotation\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'velodyne-blog' #<Your Bucket Name>\n",
    "PREFIX = 'lidar_point_cloud_data' #<Any Valid S3 Prefix>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure your bucket is in the same region as this notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket_region = s3_client.head_bucket(Bucket=BUCKET)[\"ResponseMetadata\"][\"HTTPHeaders\"][\n",
    "    \"x-amz-bucket-region\"\n",
    "]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), \"Your S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading functions\n",
    "Defining some functions to help us process data (download, parse, upload, etc.) effeciently from/to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_s3_as_stream(s3_uri):\n",
    "    s3_info = urlparse(s3_uri, allow_fragments=False)\n",
    "    response = s3_client.get_object(Bucket=s3_info.netloc, Key=s3_info.path[1:])\n",
    "    file_stream = response['Body']\n",
    "    return file_stream\n",
    "\n",
    "def load_s3_as_buffer(s3_uri):\n",
    "    s3_info = urlparse(s3_uri, allow_fragments=False)\n",
    "    content_object = s3.Object(s3_info.netloc, s3_info.path[1:])\n",
    "    file_fuffer = content_object.get()['Body'].read()\n",
    "    return file_fuffer\n",
    "\n",
    "\n",
    "def load_s3_as_string(s3_uri):\n",
    "    file_content = load_s3_as_buffer(s3_uri).decode('utf-8')\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def json_print(json_obj):\n",
    "    print(json.dumps(json_obj, sort_keys=True, default=str, indent=2))\n",
    "\n",
    "\n",
    "def load_json_from_s3(s3_uri):\n",
    "    s3_content = load_s3_as_string(s3_uri)\n",
    "    json_content = json.loads(s3_content)\n",
    "    return json_content\n",
    "\n",
    "# Doesn't work with OpenCV generated yaml file    \n",
    "def load_yaml_from_s3(s3_uri):\n",
    "    stream = load_s3_as_stream(s3_uri)\n",
    "    return yaml.safe_load(stream)\n",
    "\n",
    "def write_json_to_s3(content, bucket, key):\n",
    "    s3_object = s3.Object(bucket, key)\n",
    "    str_content = json.dumps(content, sort_keys=True, default=str, indent=2)\n",
    "    s3_object.put(Body=str_content.encode('utf-8'))\n",
    "\n",
    "\n",
    "def write_txt_to_s3(txt, bucket, key):\n",
    "    s3_object = s3.Object(bucket, key)\n",
    "    s3_object.put(Body=txt.encode('utf-8'))\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "    \n",
    "def list_s3_objects(bucket, prefix, surfix):\n",
    "    list_response = s3_client.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    if list_response[\"IsTruncated\"]:\n",
    "        print(\"There are more then 1000 files, we need to handle pagenation.\")\n",
    "        assert False\n",
    "\n",
    "    surfix_len = len(surfix)\n",
    "    all_key = [content[\"Key\"] for content in list_response[\"Contents\"] if surfix == content[\"Key\"].split(\"/\")[-1][-surfix_len:]]\n",
    "    all_key.sort()\n",
    "\n",
    "    return all_key\n",
    "\n",
    "def write_manifest_to_s3(manifest_lines, bucket, key):\n",
    "    s3_object = s3.Object(bucket, key)\n",
    "    str_content = \"\"\n",
    "    for line_json in manifest_lines:\n",
    "        str_content += (json.dumps(line_json, separators=(',', ':'))+\"\\n\")\n",
    "    s3_object.put(Body=str_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the LiDAR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Velodyne provided the LiDAR sensor dataset for this example. This data is under [Creative Commons Attribution-NonCommercial-ShareAlike 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/) license, and it is hosted at https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-8424/lidar_point_cloud_data.zip\n",
    "\n",
    "This dataset contains one continous scene from a **Autonomous vehicle experiment driving around on a highway between Oakland and San Francisco?***. The entire scene contains 60 frames. The dataset also include camera images, sensor pose, etc. in addition to the point cloud data (.pcd).  here are the data content.\n",
    "\n",
    "- lidar_cam_calib_vlp32_06_10_2021.yaml (camera calibration info, 1 camera only)\n",
    "- images/ (camera footage for each frame)\n",
    "- poses/ (pose json file containing LiDAR extrinsic matrix for each frame) \n",
    "- rectified_scans_local/ (.pcb files in LiDAR sensor local coordinate system)\n",
    "\n",
    "Run the section below to download the dataset locally and then upload to your S3 bucket which you have defined in the sections above.\n",
    "\n",
    "If you are interested, you can also ran the optional section to see the experiment details from the sensor setting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-8424/lidar_point_cloud_data.zip\n",
    "!unzip lidar_point_cloud_data.zip\n",
    "\n",
    "target_s3 = f's3://{BUCKET}/{PREFIX}'\n",
    "!aws s3 cp ./lidar_point_cloud_data $target_s3 --recursive\n",
    "\n",
    "!rm lidar_point_cloud_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcd_keys = list_s3_objects(BUCKET, f\"{PREFIX}/rectified_scans_local\", \"pcd\")\n",
    "print(f'Total number of frames in this scene is {len(all_pcd_keys)} =================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data\n",
    "Each `.pcb` file is one frame in the entire scene.  We will use the pcb.py module from pyntcloud to load the frame data. We will also take a quick peek of the data structure, this will be transformed into the Ground Truth supported data format in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pcd_key in all_pcd_keys[0:1]:\n",
    "    print(pcd_key)\n",
    "    point_data = pcd.read_pcd(pcd_key)\n",
    "    \n",
    "point_data['points'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Labeling Job\n",
    "\n",
    "Object tracking is our Task Type. read more here on other [3D Point Cloud Task Types](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-task-types.html).\n",
    "\n",
    "To create an Object Tracking Point Cloud Labeling Job, you need to feed the following resources as the labeling job inputs:\n",
    "\n",
    "1) **Create Point Cloud Sequence Input Manifest:** This is a json file defining the point cloud frame sequence and associated sensor fusion data. For more information, see [Create a Point Cloud Sequence Input Manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-multi-frame-input-data.html).\n",
    "\n",
    "2) **Create a input manifest file:** This is the input file for the labeling job. Each line of the manifest fine contains a link to a sequence file define in step 1.\n",
    "\n",
    "3) **Create a Label Category Configuration file:** This file is used to specify your labels, label category, frame atrributes, and worker instructions. For more information, see [Create a Labeling Category Configuration File](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-cat-config-attributes.html).\n",
    "\n",
    "4) **Provide Pre-defined AWS Resources**\n",
    "  - **Pre-annotation Lambda ARN:** Please refer to [PreHumanTaskLambdaArn](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#sagemaker-Type-HumanTaskConfig-PreHumanTaskLambdaArn). \n",
    "  \n",
    "  - **Annotation Consolidation ARN** This lambda function is used to consolidate labels from different workers. Please refer to [AnnotationConsolidationLambdaArn](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AnnotationConsolidationConfig.html#sagemaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn).\n",
    "  \n",
    "  - **Workforce ARN:** define which workforce type you would like to use. Please refer to [Create and Manage Workforces](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html) for more details.\n",
    "  \n",
    "  - **HumanTaskUiArn:** You must enter a HumanTaskUiArn to define the worker UI template to do the labeling job. This should like something like this `arn:aws:sagemaker:<region>:394669845002:human-task-ui/PointCloudObjectTracking`. Replace <region> your region info.\n",
    "\n",
    "\n",
    "**[Be AWARE]**\n",
    "\n",
    "- There should not be an entry for the UiTemplateS3Uri parameter.\n",
    "\n",
    "- Your LabelAttributeName must end in -ref. For example, ot-labels-ref.\n",
    "\n",
    "- The number of workers specified in NumberOfHumanWorkersPerDataObject should be 1.\n",
    "\n",
    "- <span style=\"color:red\">**3D point cloud labeling does not support active learning**</span>, so do not specify values for parameters in LabelingJobAlgorithmsConfig.\n",
    "\n",
    "- Be aware, 3D point cloud object tracking labeling jobs can take multiple hours to complete. You should specify a longer time limit for these labeling jobs in TaskTimeLimitInSeconds (up to 7 days, or 604,800 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object tracking as our 3D Point Cloud Task Type. \n",
    "task_type = \"3DPointCloudObjectTracking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud Sequence Input Manifest File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the most important steps to generating a sequence input manifest file is to 1) convert the 3D points to **world coordinate system** and 2) generate the sensor extrinsic matrix to enable **Sensor fusion** feature in SageMaker GroundTruth.  \n",
    "\n",
    "**World Coordinate System** The LiDAR sensor is mounted on a moving vehicle (ego vehicle) which captures the data in its own frame of reference.  In order to perform obnject tracking, we need to convert this data to a global frame of reference to account for the moving ege vehicle itself.\n",
    "\n",
    "**Sensor Fusion** is a feature in SaGeMaker Ground Truth that synchronizes the 3D point cloud frame side-b-side with the video freame.  This provide visual context for human labelers and also allow labelers to adjust annoation in 3D and 2D images synchronously. \n",
    "\n",
    "For the step-by-step instruction on how the matrix transformation is done, please check out this [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/labeling-data-for-3d-object-tracking-and-sensor-fusion-in-amazon-sagemaker-ground-truth/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`generate_transformed_pcd_from_point_cloud`** function performance the coordinate translation and then generate the 3D point data file which Ground Truth can consume.\n",
    "\n",
    "**Coordinate Translation**: To Translate the data from local/sensor global coordinate system, multiplying each point in a 3D frame with the extrinsic matrix for the LiDAR sensor.\n",
    "\n",
    "**Raw 3D Data File format**: Ground Truthes renders the 3D point cloud data in either Compact Binary Pack (.bin) or ASCII (.txt) format.  File in these format need to contain information about the location (x, y, and z coordinates) of all points that make up that frame, and, optionally, information about the pixel color of each point for colored point clouds (i, r, g, b).\n",
    "\n",
    "To read more about Ground Truth accepted raw 3d data formats, see [Raw 3D Data Formats](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-raw-data-types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate the transformed point cloud file for GT.  \n",
    "def generate_transformed_pcd_from_point_cloud(points, lidar_extrinsic_matrix, tranform = False):\n",
    "    tps = []\n",
    "    lidar_txt = \"\"\n",
    "\n",
    "    for point in points:\n",
    "        \n",
    "        if tranform: #transform from local to global coordinate system and then generate the txt file\n",
    "            transformed_points = np.matmul(lidar_extrinsic_matrix, np.array([point[0], point[1], point[2], 1], dtype=np.float32).reshape(4,1)).tolist()\n",
    "            if len(point) > 3 and point[3] is not None:\n",
    "                tps.append([transformed_points[0][0], transformed_points[1][0], transformed_points[2][0], point[3]])\n",
    "                pctxt = f\"{transformed_points[0][0]} {transformed_points[1][0]} {transformed_points[2][0]} {point[3]}\"\n",
    "                lidar_txt = lidar_txt + pctxt + '\\n'\n",
    "        else: # else generate txt file only [Test Purpose]\n",
    "            if len(point) > 3 and point[3] is not None:\n",
    "                pctxt = f\"{point[0]} {point[1]} {point[2]} {point[3]}\"\n",
    "                lidar_txt = lidar_txt + pctxt + '\\n'\n",
    "                \n",
    "    return lidar_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below extracts pose data from camera extrinsic matrices to populate the 3D point cloud sequence input manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# utility to convert extrinsic matrix to pose heading quaternion and position\n",
    "def convert_extrinsic_matrix_to_trans_quaternion_mat(lidar_extrinsic_transform):\n",
    "    position = lidar_extrinsic_transform[0:3, 3]\n",
    "    rot = np.linalg.inv(lidar_extrinsic_transform[0:3, 0:3])\n",
    "    quaternion= R.from_matrix(np.asarray(rot)).as_quat()\n",
    "    trans_quaternions = {\n",
    "        \"translation\": {\n",
    "            \"x\": position[0],\n",
    "            \"y\": position[1],\n",
    "            \"z\": position[2]\n",
    "        },\n",
    "        \"rotation\": {\n",
    "            \"qx\": quaternion[0],\n",
    "            \"qy\": quaternion[1],\n",
    "            \"qz\": quaternion[2],\n",
    "            \"qw\": quaternion[3]\n",
    "            }\n",
    "    }\n",
    "    return trans_quaternions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_camera_inv_extrinsic_matrix_to_trans_quaternion_mat(camera_extrinsic_transform):\n",
    "    position = camera_extrinsic_transform[0:3, 3]\n",
    "    rot = np.linalg.inv(camera_extrinsic_transform[0:3, 0:3])\n",
    "    quaternion= R.from_matrix(np.asarray(rot)).as_quat()\n",
    "    trans_quaternions = {\n",
    "        \"translation\": {\n",
    "            \"x\": position[0],\n",
    "            \"y\": position[1],\n",
    "            \"z\": position[2]\n",
    "        },\n",
    "        \"rotation\": {\n",
    "            \"qx\": quaternion[0],\n",
    "            \"qy\": quaternion[1],\n",
    "            \"qz\": quaternion[2],\n",
    "            \"qw\": -quaternion[3]\n",
    "            }\n",
    "    }\n",
    "    return trans_quaternions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sequence Input Manifest File\n",
    "\n",
    "The code below performs the following steps to build the Point Cloud Sequence Inpute Manifest File\n",
    "\n",
    "1) Load data \n",
    "    - Point cloud data from .pcd file\n",
    "    - LiDAR extrinsic Matrix from the pose file\n",
    "    - Camera extrinsic, intrinsic, and distortion data from the camera calibration yaml file\n",
    "\n",
    "2) Per frame, Transform the raw point cloud to the global frame of reference. Generate and store ASCII (.txt) for each frame to S3 \n",
    "\n",
    "3) Extract ego vehicle pose from LiDAR extrinsic Matrix\n",
    "\n",
    "4) build sensor poistion in global coordinate system by extracting camera pose from camera inverse Extrinsic\n",
    "\n",
    "5) provide camera calibration parameters (distortion, skew, etc.)\n",
    "\n",
    "6) build the array of data frames: ref the ASCII file location, define the vehicle position in world coordinate system, etc.\n",
    "\n",
    "7) create the sequence manifest file: sequence.json\n",
    "\n",
    "8) create our input manifest file.  each line identifies a singel sequence file we just uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "# recalibrate the seq file\n",
    "calib_yaml = load_yaml_from_s3(f's3://{BUCKET}/{PREFIX}/lidar_cam_calib_vlp32_06_10_2021.yaml')\n",
    "\n",
    "print(calib_yaml[\"cam0\"][\"ext_R\"])\n",
    "\n",
    "cam_r = Rotation.from_euler('zyx', calib_yaml[\"cam0\"][\"ext_R\"], degrees=True)\n",
    "camera_extrinsic_calibrations = np.append(cam_r.as_matrix(), np.asarray(calib_yaml[\"cam0\"][\"ext_t\"]).reshape(3,1), 1)\n",
    "camera_extrinsic_calibrations = np.append(camera_extrinsic_calibrations, np.asarray([0, 0, 0, 1]).reshape(1,4), 0)\n",
    "\n",
    "camera_intrinsics = calib_yaml[\"cam0\"][\"intrinsics\"]\n",
    "camera_distortion = calib_yaml[\"cam0\"][\"distortion_coeffs\"]\n",
    "\n",
    "# print(camera_extrinsic_calibrations)\n",
    "# get file name of all the frames\n",
    "frame_s3_keys = [content.split(\"/\")[-1].split(\".\")[0] for content in all_pcd_keys \n",
    "               if content.split(\"/\")[-1].endswith(\".pcd\")]\n",
    "frame_s3_keys.sort()\n",
    "\n",
    "seq_json = {}\n",
    "seq_json[\"seq-no\"] = 1\n",
    "seq_json[\"prefix\"] = f\"s3://{BUCKET}/{PREFIX}/\"\n",
    "seq_json[\"number-of-frames\"] = len(frame_s3_keys)\n",
    "seq_json[\"frames\"] = []\n",
    "\n",
    "for idx, frame_key in enumerate(all_pcd_keys):\n",
    "    \n",
    "    if frame_key.split(\"/\")[-1].endswith(\".pcd\"):\n",
    "        frame_id = frame_key.split(\"/\")[-1].split(\".\")[0]\n",
    "                \n",
    "        # load the data points from pcb file\n",
    "        points = pcd.read_pcd(frame_key)[\"points\"].to_numpy(dtype=np.dtype(object))[:,[2,3,4,5]]\n",
    "\n",
    "        # Each pose file is the pose of the LiDAR sensors. This is the LiDAR Extrinsic Matrix\n",
    "        # used to rotate sensor data from local to global.\n",
    "        lidar_pose = load_json_from_s3(f\"s3://{BUCKET}/{PREFIX}/poses/{frame_id}.json\")   \n",
    "        # next 2 steps build generates the lidar_extrinsic_matrix\n",
    "        lidar_extrinsic_matrix = np.append(lidar_pose[\"rotation\"], np.asarray(lidar_pose[\"translation\"]).reshape(3,1), 1)\n",
    "        lidar_extrinsic_matrix = np.append(lidar_extrinsic_matrix, np.asarray([0, 0, 0, 1]).reshape(1,4), 0)\n",
    "\n",
    "\n",
    "        trans_quaternions = convert_extrinsic_matrix_to_trans_quaternion_mat(lidar_extrinsic_matrix)\n",
    "        \n",
    "        # customer transforms points from lidar to global frame using lidar_extrinsic_matrix\n",
    "        transformed_pcl_txt = generate_transformed_pcd_from_point_cloud(points, lidar_extrinsic_matrix, tranform=True)\n",
    "        \n",
    "        lidar_txt_key = frame_key.replace(\".pcd\", \".txt\").replace(\"rectified_scans_local\", \"rectified_txt_global\")\n",
    "\n",
    "        txt_s3_path = write_txt_to_s3(transformed_pcl_txt, BUCKET, lidar_txt_key)\n",
    "\n",
    "        ego_vehicle_pose = {}\n",
    "        ego_vehicle_pose['heading'] = trans_quaternions['rotation']\n",
    "        ego_vehicle_pose['position'] = trans_quaternions['translation']\n",
    "\n",
    "        frame = dict()\n",
    "        frame[\"frame-no\"] = idx\n",
    "        frame[\"frame\"] = f\"rectified_txt_global/{frame_id}.txt\"\n",
    "        frame[\"format\"] = \"text/xyzi\"\n",
    "        frame[\"unix-timestamp\"] = int(frame_id.split(\"_\")[-1])/1000000000\n",
    "        frame[\"ego-vehicle-pose\"] = ego_vehicle_pose\n",
    "\n",
    "        images = []\n",
    "\n",
    "        # There is only one camera, so only one image_json per frame\n",
    "        image_json = dict()\n",
    "\n",
    "        # Camera Extrinsic Calibration comes from ext_R (rotation) and ext_t (translation) of the \n",
    "        # lidar cam calib yaml file\n",
    "        camera_transform= np.linalg.inv(np.matmul(camera_extrinsic_calibrations, np.linalg.inv(lidar_extrinsic_matrix)))\n",
    "        \n",
    "\n",
    "        cam_trans_quaternions = convert_camera_inv_extrinsic_matrix_to_trans_quaternion_mat(camera_transform)\n",
    "    \n",
    "        image_json[\"image-path\"] = f\"images/{frame_id}.jpeg\"\n",
    "        image_json[\"unix-timestamp\"] = int(frame_id.split(\"_\")[-1])/1000000000\n",
    "        image_json['heading'] = cam_trans_quaternions['rotation']\n",
    "        image_json['position'] = cam_trans_quaternions['translation']\n",
    "        image_json[\"camera_model\"] =  \"pinhole\" # All image already undistorted\n",
    "\n",
    "        # Camera Intrinsic matrix from lidar cam calib yaml file\n",
    "        image_json['fx'] = camera_intrinsics[0]\n",
    "        image_json['fy'] = camera_intrinsics[1]\n",
    "        image_json['cx'] = camera_intrinsics[2]\n",
    "        image_json['cy'] = camera_intrinsics[3]\n",
    "        # Camera distortion from lidar cam calib yaml file\n",
    "        image_json['k1'] = camera_distortion[0]\n",
    "        image_json['k2'] = camera_distortion[1]\n",
    "        image_json['k3'] = camera_distortion[2]\n",
    "        image_json['k4'] = camera_distortion[3]\n",
    "        # no tangential distortion\n",
    "        image_json['p1'] = 0\n",
    "        image_json['p2'] = 0\n",
    "        # no skew\n",
    "        image_json['skew'] = 0\n",
    "\n",
    "        images.append(image_json)\n",
    "\n",
    "        frame[\"images\"] = images\n",
    "\n",
    "        seq_json['frames'].append(frame)\n",
    "\n",
    "\n",
    "seq_key = f\"{PREFIX}/manifests_categories/sequence.json\"\n",
    "print(f\"Creating sequence file: s3://{BUCKET}/{seq_key}\")\n",
    "write_json_to_s3(seq_json, BUCKET, seq_key)\n",
    "\n",
    "# Building the input manifest file reference the sequences.  \n",
    "# In this case we are only referencing one sequence.\n",
    "manifest_line = [{\n",
    "    \"source-ref\": f\"s3://{BUCKET}/{seq_key}\",\n",
    "}]\n",
    "manifest_key = f\"{PREFIX}/manifests_categories/manifest.json\"\n",
    "write_manifest_to_s3(manifest_line, BUCKET, manifest_key)\n",
    "\n",
    "manifest_uri = f\"s3://{BUCKET}/{manifest_key}\"\n",
    "\n",
    "print(f\"Creating manifest file: {manifest_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Category Configuration File\n",
    "\n",
    "Your label category configuration file is used to specify labels, or classes, for your labeling job.\n",
    "\n",
    "When you use the object detection or object tracking task types, you can also include label attributes in your [label category configuration file](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-cat-config-attributes.html). Workers can assign one or more attributes you provide to annotations to give more information about that object. For example, you may want to use the attribute occluded to have workers identify when an object is partially obstructed.\n",
    "\n",
    "Let's look at an example of the label category configuration file for an object detection or object tracking labeling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_category = {\n",
    "  \"categoryGlobalAttributes\": [\n",
    "    {\n",
    "      \"enum\": [\n",
    "        \"75-100%\",\n",
    "        \"25-75%\",\n",
    "        \"0-25%\"\n",
    "      ],\n",
    "      \"name\": \"Visibility\",\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  ],\n",
    "  \"documentVersion\": \"2020-03-01\",\n",
    "  \"instructions\": {\n",
    "    \"fullInstruction\": \"Draw a tight Cuboid. You only need to annotate those in the first frame. Please make sure the direction of the cubiod is accurately representative of the direction of the vehicle it bounds.\",\n",
    "    \"shortInstruction\": \"Draw a tight Cuboid. You only need to annotate those in the first frame.\"\n",
    "  },\n",
    "  \"labels\": [\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Car\"\n",
    "    },\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Truck\"\n",
    "    },\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Bus\"\n",
    "    },\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Pedestrian\"\n",
    "    },\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Cyclist\"\n",
    "    },\n",
    "    {\n",
    "      \"categoryAttributes\": [],\n",
    "      \"label\": \"Motorcyclist\"\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "\n",
    "category_key = f'{PREFIX}/manifests_categories/label_category.json'\n",
    "write_json_to_s3(label_category, BUCKET, category_key)\n",
    "\n",
    "label_category_file = f's3://{BUCKET}/{category_key}'\n",
    "print(f\"label category file uri: {label_category_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the job resources\n",
    "\n",
    "### Human Task UI ARN\n",
    "\n",
    "[HumanTaskUiArn](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UiConfig.html) is an resource that defines the worker task template used to render the worker UI and tools for labeling job.  This attribute is defined under `UiConfig` and the resource name is configured by region and task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_task_ui_arn = (\n",
    "    f\"arn:aws:sagemaker:{region}:394669845002:human-task-ui/{task_type[2:]}\"\n",
    ")\n",
    "human_task_ui_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your work resource\n",
    "\n",
    "In this example, we will use private team resources.  Please follow this instruction to create a [private workforce ](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-cognito.html).  Once you are done, put your resource ARN in the parameter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteam_arn = f\"arn:aws:sagemaker:{region}:259883177473:workteam/private-crowd/test-team\"#\"<REPLACE W/ YOUR Private Team ARN>\"\n",
    "workteam_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-annotation Lambda ARN and Post-annotation Lambda ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\",\n",
    "    \"ap-northeast-1\": \"477331159723\",\n",
    "}\n",
    "\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-{}\".format(region, ac_arn_map[region], task_type)\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-{}\".format(region, ac_arn_map[region], task_type)\n",
    "\n",
    "print(acs_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up HumanTaskConfig\n",
    "\n",
    "This is used to specify your work team,a nd configure your labeling job task.  Feel free to update the task description info below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"velodyne-blog-test-{str(time.time()).split('.')[0]}\"\n",
    "\n",
    "# Task description info =================\n",
    "task_description = \"Draw 3D boxes around required objects\"\n",
    "task_keywords = ['lidar', 'pointcloud']\n",
    "task_title = job_name\n",
    "\n",
    "human_task_config = {\n",
    "    \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "    },\n",
    "    \"WorkteamArn\": workteam_arn,\n",
    "    \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "    \"MaxConcurrentTaskCount\": 200,\n",
    "    \"NumberOfHumanWorkersPerDataObject\": 1,  # One worker will work on each task\n",
    "    \"TaskAvailabilityLifetimeInSeconds\": 18000, # Your workteam has 5 hours to complete all pending tasks.\n",
    "    \"TaskDescription\": task_description,\n",
    "    \"TaskKeywords\": task_keywords,\n",
    "    \"TaskTimeLimitInSeconds\": 36000, # Each seq must be labeled within 1 hour.\n",
    "    \"TaskTitle\": task_title,\n",
    "    \"UiConfig\": {\n",
    "        \"HumanTaskUiArn\": human_task_ui_arn,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Create Labeling Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelAttributeName = f\"{job_name}-ref\" #must end with -ref\n",
    "\n",
    "output_path = f\"s3://{BUCKET}/{PREFIX}/output\"\n",
    "\n",
    "ground_truth_request = {\n",
    "    \"InputConfig\" : {\n",
    "      \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "          \"ManifestS3Uri\": manifest_uri,\n",
    "        }\n",
    "      },\n",
    "      \"DataAttributes\": {\n",
    "        \"ContentClassifiers\": [\n",
    "          \"FreeOfPersonallyIdentifiableInformation\",\n",
    "          \"FreeOfAdultContent\"\n",
    "        ]\n",
    "      },  \n",
    "    },\n",
    "    \"OutputConfig\" : {\n",
    "      \"S3OutputPath\": output_path,\n",
    "    },\n",
    "    \"HumanTaskConfig\" : human_task_config,\n",
    "    \"LabelingJobName\": job_name,\n",
    "    \"RoleArn\": role, \n",
    "    \"LabelAttributeName\": labelAttributeName,\n",
    "    \"LabelCategoryConfigS3Uri\": label_category_file,\n",
    "    \"Tags\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call CreateLabelingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.create_labeling_job(**ground_truth_request)\n",
    "print(f'Labeling Job created: {job_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Status of Labeling Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call describeLabelingJob\n",
    "describeLabelingJob = sagemaker_client.describe_labeling_job(LabelingJobName=job_name)\n",
    "print(describeLabelingJob['LabelingJobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Working on tasks\n",
    "\n",
    "When you labeling job is ready, add yourself to your private work team and experiment with the worker's portal.  You should receive an email with the portal link, your username, and a temporary password.  When you login, select the labeling job from the list, and you should see the worker's portal like this.  (Note: it may take a few minutes for a new labeling job to show up in the portal)\n",
    "\n",
    "![Labeling View 1](statics/behind_low.gif)\n",
    "![Labeling View 2](statics/side_low.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Output Data\n",
    "\n",
    "Once you are done with the labeling job, click **Submit**, you can then view the output data in the S3 output location you specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Special thanks to Velodyne team for letting us use this dataset and demonstrate 3D point cloud labeling using SageMaker Ground Truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
